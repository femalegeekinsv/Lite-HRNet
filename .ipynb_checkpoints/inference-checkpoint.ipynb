{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lite-HRNet Inference Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code has been hidden for brevity. Use the button below to show/hide code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Show/Hide Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Show/Hide Code\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import mmcv\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from mmcv import Config\n",
    "from mmcv.cnn import fuse_conv_bn\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "from models import build_posenet\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import HBox, Label, Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript, display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "def run_cells_below(b):\n",
    "    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.get_selected_index()+2)'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_IMG_TYPES = ['jpg','jpeg', 'png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d540e51072f24dd5a6e292de7cae76b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Input image path:'), Text(value='../data/ESTEC_Images/image_95.png', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "txt_input_img = widgets.Text(\n",
    "    value= \"../data/ESTEC_Images/image_95.png\",\n",
    "    placeholder=\"Input image path\",\n",
    "    layout= Layout(width='60%')\n",
    "    )\n",
    "\n",
    "HBox([Label('Input image path:'),txt_input_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0122911180f4408b6ae6b8a9a4c38ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Bounding box data file:'), Text(value='../data/ESTEC_Images/bbox.txt', layout=Layo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "txt_bbox = widgets.Text(\n",
    "    value= \"../data/ESTEC_Images/bbox.txt\",\n",
    "    placeholder=\"Bounding Box data file\",\n",
    "    layout= Layout(width='60%')\n",
    "    )\n",
    "\n",
    "HBox([Label('Bounding box data file:'),txt_bbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47386bdfc8dc499896e6f0b13d23e945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Load Input Image and Data', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1418b78b30a4ebbbe258e9702e443f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4138378a1882405ba3c37172c6f2ce16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_path = os.path.abspath(txt_input_img.value)\n",
    "bbox_path = os.path.abspath(txt_bbox.value)\n",
    "inputs= {}\n",
    "\n",
    "img_name = img_path.split(\"/\")[-1]\n",
    "img_id= int((img_name.split('_')[-1]).split('.')[0])  \n",
    "\n",
    "\n",
    "\n",
    "button = widgets.Button(description=\"Load Input Image and Data\")\n",
    "output1 = widgets.Output()\n",
    "output2 = widgets.Output()\n",
    "\n",
    "display(button, output1, output2)\n",
    "\n",
    "def load_image_gt(b):\n",
    "\n",
    "    ## LOAD IMAGE\n",
    "    with output1:\n",
    "        print(\"Loading... This might take a few seconds depending on the image size.\")\n",
    "    with output2:\n",
    "        \n",
    "        input_img = Image.open(img_path).convert('RGB')\n",
    "        inputs['image'] = input_img        \n",
    "\n",
    "        ## LOAD DATA\n",
    "        bbox_f = open(bbox_path,'r')\n",
    "        raw_data = bbox_f.readlines()\n",
    "        this_idx = img_id-14 ## TODO: This is very bad\n",
    "        inputs['bbox'] = [int(float(v)) for v in (raw_data[this_idx]).split(' ')]\n",
    "\n",
    "        ## Visualize GT image/bbox\n",
    "        vis_img = np.array(input_img)\n",
    "        xyxy =inputs['bbox']\n",
    "        #relax margins\n",
    "        ext_scale = 0.10 # percent of bbox h/w\n",
    "        w = abs(xyxy[2]-xyxy[0])\n",
    "        h = abs(xyxy[3]-xyxy[1])\n",
    "        xyxy = [int(xyxy[0]+ (w*ext_scale)), int(xyxy[1]+ (h*ext_scale)), int(xyxy[2]- (w*ext_scale)), int(xyxy[3]-(h*ext_scale))]\n",
    "        cv2.rectangle(vis_img, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (255,255,255), 10)\n",
    "        \n",
    "\n",
    "        imgvis = Image.fromarray(vis_img, 'RGB')\n",
    "        display(imgvis)\n",
    "        \n",
    "\n",
    "        # RESET CAPTION\n",
    "        with output1:\n",
    "            clear_output()\n",
    "            print(f\"Input Image: {img_name}; ID: {img_id}; Bounding box: {xyxy}\")\n",
    "\n",
    "        \n",
    "    \n",
    "button.on_click(load_image_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334652c6fcbf446889218307c2b3984e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Bounding box data file:'), Text(value='configs/top_down/lite_hrnet/Envisat/litehrn…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt_config = widgets.Text(\n",
    "    value= \"configs/top_down/lite_hrnet/Envisat/litehrnet_18_coco_256x256_Envisat+IC.py\",\n",
    "    placeholder=\"Config\",\n",
    "    layout= Layout(width='90%')\n",
    "    )\n",
    "\n",
    "HBox([Label('Bounding box data file:'),txt_config])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ad3fe757754a50a304f04c087d7511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Bounding box data file:'), Text(value='work_dirs/litehrnet_18_coco_256x256_Envisat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "txt_ckpt = widgets.Text(\n",
    "    value= \"work_dirs/litehrnet_18_coco_256x256_Envisat+IC/best/best.pth\",\n",
    "    placeholder=\"Config\",\n",
    "    layout= Layout(width='90%')\n",
    "    )\n",
    "HBox([Label('Bounding box data file:'),txt_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmpose.apis.inference import LoadImage,_box2cs\n",
    "from mmpose.datasets.pipelines import Compose\n",
    "import mmpose.apis.inference as inference_module\n",
    "from mmcv.parallel import collate, scatter\n",
    "\n",
    "def new_inference_single_pose_model(model,\n",
    "                                 img_or_path,\n",
    "                                 bbox,\n",
    "                                 dataset,\n",
    "                                 return_heatmap=False):\n",
    "    \"\"\"Inference a single bbox.\n",
    "\n",
    "    num_keypoints: K\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The loaded pose model.\n",
    "        img_or_path (str | np.ndarray): Image filename or loaded image.\n",
    "        bbox (list | np.ndarray): Bounding boxes (with scores),\n",
    "            shaped (4, ) or (5, ). (left, top, width, height, [score])\n",
    "        dataset (str): Dataset name.\n",
    "        outputs (list[str] | tuple[str]): Names of layers whose output is\n",
    "            to be returned, default: None\n",
    "\n",
    "    Returns:\n",
    "        ndarray[Kx3]: Predicted pose x, y, score.\n",
    "        heatmap[N, K, H, W]: Model output heatmap.\n",
    "    \"\"\"\n",
    "\n",
    "    cfg = model.cfg\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # build the data pipeline\n",
    "    channel_order = cfg.test_pipeline[0].get('channel_order', 'rgb')\n",
    "    test_pipeline = [LoadImage(channel_order=channel_order)\n",
    "                     ] + cfg.test_pipeline[1:]\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "\n",
    "    assert len(bbox) in [4, 5]\n",
    "    center, scale = _box2cs(cfg, bbox)\n",
    "\n",
    "    flip_pairs = None\n",
    "    if dataset == 'TopDownEnvisatCocoDataset':\n",
    "        flip_pairs = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 0]]\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # prepare data\n",
    "    data = {\n",
    "        'img_or_path':\n",
    "        img_or_path,\n",
    "        'center':\n",
    "        center,\n",
    "        'scale':\n",
    "        scale,\n",
    "        'bbox_score':\n",
    "        bbox[4] if len(bbox) == 5 else 1,\n",
    "        'dataset':\n",
    "        dataset,\n",
    "        'joints_3d':\n",
    "        np.zeros((cfg.data_cfg.num_joints, 3), dtype=np.float32),\n",
    "        'joints_3d_visible':\n",
    "        np.zeros((cfg.data_cfg.num_joints, 3), dtype=np.float32),\n",
    "        'rotation':\n",
    "        0,\n",
    "        'ann_info': {\n",
    "            'image_size': cfg.data_cfg['image_size'],\n",
    "            'num_joints': cfg.data_cfg['num_joints'],\n",
    "            'flip_pairs': flip_pairs\n",
    "        }\n",
    "    }\n",
    "    data = test_pipeline(data)\n",
    "    data = collate([data], samples_per_gpu=1)\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        # scatter to specified GPU\n",
    "        data = scatter(data, [device])[0]\n",
    "    else:\n",
    "        # just get the actual data from DataContainer\n",
    "        data['img_metas'] = data['img_metas'].data[0]\n",
    "\n",
    "    # forward the model\n",
    "    with torch.no_grad():\n",
    "        result = model(\n",
    "            img=data['img'],\n",
    "            img_metas=data['img_metas'],\n",
    "            return_loss=False,\n",
    "            return_heatmap=return_heatmap)\n",
    "\n",
    "    return result['preds'][0], result['output_heatmap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n",
      "Use load_from_local loader\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 12 is out of bounds for axis 1 with size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-90e9dae74a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_pose_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_top_down_pose_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_data_list_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xyxy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TopDownEnvisatCocoDataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/lite_hrnet/lib/python3.8/site-packages/mmpose/apis/inference.py\u001b[0m in \u001b[0;36minference_top_down_pose_model\u001b[0;34m(model, img_or_path, person_results, bbox_thr, format, dataset, return_heatmap, outputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             pose, heatmap = _inference_single_pose_model(\n\u001b[0m\u001b[1;32m    351\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mimg_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-590e71c490b5>\u001b[0m in \u001b[0;36mnew_inference_single_pose_model\u001b[0;34m(model, img_or_path, bbox, dataset, return_heatmap)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# forward the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         result = model(\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mimg_metas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_metas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lite_hrnet/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lite_hrnet/lib/python3.8/site-packages/mmpose/models/detectors/top_down.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, target, target_weight, img_metas, return_loss, return_heatmap, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m             return self.forward_train(img, target, target_weight, img_metas,\n\u001b[1;32m    128\u001b[0m                                       **kwargs)\n\u001b[0;32m--> 129\u001b[0;31m         return self.forward_test(\n\u001b[0m\u001b[1;32m    130\u001b[0m             img, img_metas, return_heatmap=return_heatmap, **kwargs)\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lite_hrnet/lib/python3.8/site-packages/mmpose/models/detectors/top_down.py\u001b[0m in \u001b[0;36mforward_test\u001b[0;34m(self, img, img_metas, return_heatmap, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mfeatures_flipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_flipped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_keypoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 output_flipped_heatmap = self.keypoint_head.inference_model(\n\u001b[0m\u001b[1;32m    175\u001b[0m                     features_flipped, img_metas[0]['flip_pairs'])\n\u001b[1;32m    176\u001b[0m                 output_heatmap = (output_heatmap +\n",
      "\u001b[0;32m~/miniconda3/envs/lite_hrnet/lib/python3.8/site-packages/mmpose/models/keypoint_heads/top_down_simple_head.py\u001b[0m in \u001b[0;36minference_model\u001b[0;34m(self, x, flip_pairs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mflip_pairs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             output_heatmap = flip_back(\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mflip_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lite_hrnet/lib/python3.8/site-packages/mmpose/core/post_processing/post_transforms.py\u001b[0m in \u001b[0;36mflip_back\u001b[0;34m(output_flipped, flip_pairs, target_type)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# Swap left-right parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflip_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0moutput_flipped_back\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_flipped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0moutput_flipped_back\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_flipped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0moutput_flipped_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_flipped_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_ori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 12 is out of bounds for axis 1 with size 12"
     ]
    }
   ],
   "source": [
    "\n",
    "inference_module._inference_single_pose_model = new_inference_single_pose_model\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else None\n",
    "\n",
    "config_path = txt_config.value\n",
    "ckpt_path = txt_ckpt.value\n",
    "inputs['name'] = img_name\n",
    "inputs['id'] = img_id\n",
    "img_data_list_dict = [inputs]\n",
    "\n",
    "cfg = Config.fromfile(config_path)\n",
    "model = build_posenet(cfg.model)\n",
    "load_checkpoint(model, ckpt_path, map_location='cpu')\n",
    "model = inference_module.init_pose_model(config_path, ckpt_path, device=device)\n",
    "\n",
    "output = inference_module.inference_top_down_pose_model(model, img_path, img_data_list_dict, format='xyxy', dataset='TopDownEnvisatCocoDataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('lite_hrnet': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0a2c41ad191e4969543f3f1a29c8b10a0a42e28256af1b3e21ffcd57a58ac79a9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "a2c41ad191e4969543f3f1a29c8b10a0a42e28256af1b3e21ffcd57a58ac79a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
